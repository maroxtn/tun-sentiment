{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-02T13:52:03.896453Z",
     "iopub.status.busy": "2021-06-02T13:52:03.896085Z",
     "iopub.status.idle": "2021-06-02T13:52:47.3357Z",
     "shell.execute_reply": "2021-06-02T13:52:47.334231Z",
     "shell.execute_reply.started": "2021-06-02T13:52:03.896422Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset, Sampler\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:52:47.338591Z",
     "iopub.status.busy": "2021-06-02T13:52:47.338117Z",
     "iopub.status.idle": "2021-06-02T13:52:47.344119Z",
     "shell.execute_reply": "2021-06-02T13:52:47.342676Z",
     "shell.execute_reply.started": "2021-06-02T13:52:47.338542Z"
    }
   },
   "outputs": [],
   "source": [
    "#The important parameters\n",
    "model_name_ar = 'moha/arabert_c19'\n",
    "model_name_en = 'bert-base-uncased'\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 3\n",
    "\n",
    "seed = 99 #Important for reproducing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:52:47.345896Z",
     "iopub.status.busy": "2021-06-02T13:52:47.345589Z",
     "iopub.status.idle": "2021-06-02T13:53:01.181887Z",
     "shell.execute_reply": "2021-06-02T13:53:01.180701Z",
     "shell.execute_reply.started": "2021-06-02T13:52:47.345866Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_ar = AutoTokenizer.from_pretrained(model_name_ar, do_lower_case=True)\n",
    "tokenizer_en = AutoTokenizer.from_pretrained(model_name_en, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:01.192636Z",
     "iopub.status.busy": "2021-06-02T13:53:01.192236Z",
     "iopub.status.idle": "2021-06-02T13:53:01.207015Z",
     "shell.execute_reply": "2021-06-02T13:53:01.206017Z",
     "shell.execute_reply.started": "2021-06-02T13:53:01.192598Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:01.209287Z",
     "iopub.status.busy": "2021-06-02T13:53:01.208855Z",
     "iopub.status.idle": "2021-06-02T13:53:03.987754Z",
     "shell.execute_reply": "2021-06-02T13:53:03.986452Z",
     "shell.execute_reply.started": "2021-06-02T13:53:01.209241Z"
    }
   },
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(\"data/transliteration/dataset.xlsx\")\n",
    "dataset = pd.read_excel(xls, \"Sheet1\")\n",
    "\n",
    "known = dataset[dataset.from_source == True]\n",
    "dataset = dataset[[\"arabizi\", \"arabic\", \"from_source\"]]\n",
    "\n",
    "dataset.columns = [\"Arabize\", \"Arabic\", \"from_source\"]\n",
    "\n",
    "#dataset.  #Drop Arabic duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:03.989821Z",
     "iopub.status.busy": "2021-06-02T13:53:03.989368Z",
     "iopub.status.idle": "2021-06-02T13:53:04.003371Z",
     "shell.execute_reply": "2021-06-02T13:53:04.001919Z",
     "shell.execute_reply.started": "2021-06-02T13:53:03.989771Z"
    }
   },
   "outputs": [],
   "source": [
    "#Store known words so we just replace them instead of computing them with model again\n",
    "#This saves up computation time, and improves transliteration accuracy\n",
    "known = known[[\"arabizi\", \"arabic\"]].set_index(\"arabizi\", drop=True).arabic.to_dict()\n",
    "known_idx = list(known.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:04.005475Z",
     "iopub.status.busy": "2021-06-02T13:53:04.005102Z",
     "iopub.status.idle": "2021-06-02T13:53:04.594768Z",
     "shell.execute_reply": "2021-06-02T13:53:04.593354Z",
     "shell.execute_reply.started": "2021-06-02T13:53:04.005438Z"
    }
   },
   "outputs": [],
   "source": [
    "in_max = dataset.apply(lambda x: len(str(x.Arabize)), axis=1).max()\n",
    "out_max = dataset.apply(lambda x: len(x.Arabic), axis=1).max() + 2  #Take into account eos and sos\n",
    "\n",
    "pad_token = 0\n",
    "eos_token = 2\n",
    "sos_token = 1\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:04.596889Z",
     "iopub.status.busy": "2021-06-02T13:53:04.596542Z",
     "iopub.status.idle": "2021-06-02T13:53:04.605661Z",
     "shell.execute_reply": "2021-06-02T13:53:04.604277Z",
     "shell.execute_reply.started": "2021-06-02T13:53:04.596855Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(a):\n",
    "        \n",
    "    x = a.copy()  \n",
    "    \n",
    "    def filter_letters_arabizi(word):\n",
    "        \n",
    "        word = word.replace(\"$\", \"s\")\n",
    "        word = word.replace(\"å\", \"a\")\n",
    "        word = word.replace(\"é\", \"e\")\n",
    "        word = word.replace(\"ê\", \"e\")\n",
    "        word = word.replace(\"ÿ\", \"y\")\n",
    "        word = word.replace(\"ą\", \"a\")\n",
    "        word = word.replace(\"ī\", \"i\")\n",
    "        word = word.replace(\"\\n\", \"\")\n",
    "        word = word.replace(\"′\", \"'\")\n",
    "        \n",
    "        return word\n",
    "    \n",
    "    x.Arabize = filter_letters_arabizi(str(x.Arabize))\n",
    "    x.Arabic = x.Arabic\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:04.608063Z",
     "iopub.status.busy": "2021-06-02T13:53:04.607601Z",
     "iopub.status.idle": "2021-06-02T13:53:08.040548Z",
     "shell.execute_reply": "2021-06-02T13:53:08.039363Z",
     "shell.execute_reply.started": "2021-06-02T13:53:04.608016Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset[[\"Arabize\",\"Arabic\"]] = dataset[[\"Arabize\",\"Arabic\"]].apply(preprocess, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:08.041925Z",
     "iopub.status.busy": "2021-06-02T13:53:08.041652Z",
     "iopub.status.idle": "2021-06-02T13:53:08.059417Z",
     "shell.execute_reply": "2021-06-02T13:53:08.058574Z",
     "shell.execute_reply.started": "2021-06-02T13:53:08.041898Z"
    }
   },
   "outputs": [],
   "source": [
    "in_tokens = set(\" \".join(dataset.Arabize.values.tolist()).lower())\n",
    "in_token_to_int = {token: (i+1) for i,token in enumerate(sorted(in_tokens))}\n",
    "\n",
    "in_token_to_int[0] = \"<pad>\"\n",
    "\n",
    "out_tokens = set(\" \".join(dataset.Arabic.values.tolist()))\n",
    "out_token_to_int = {token: (i+3) for i,token in enumerate(sorted(out_tokens))}\n",
    "\n",
    "\n",
    "\n",
    "out_token_to_int[\"<pad>\"] = pad_token\n",
    "\n",
    "out_token_to_int[\"<sos>\"] = sos_token\n",
    "out_token_to_int[\"<eos>\"] = eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:08.060843Z",
     "iopub.status.busy": "2021-06-02T13:53:08.060405Z",
     "iopub.status.idle": "2021-06-02T13:53:08.147449Z",
     "shell.execute_reply": "2021-06-02T13:53:08.145903Z",
     "shell.execute_reply.started": "2021-06-02T13:53:08.060765Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(a):\n",
    "    \n",
    "    x = a.copy()\n",
    "    \n",
    "    x.Arabize = [in_token_to_int[i] for i in x.Arabize.lower()]\n",
    "    x.Arabic = [sos_token] + [out_token_to_int[i] for i in x.Arabic] + [eos_token]\n",
    "    \n",
    "    x.Arabize = x.Arabize + (in_max - len(x.Arabize)) * [pad_token] \n",
    "    x.Arabic = x.Arabic + (out_max - len(x.Arabic)) * [pad_token] \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:08.149015Z",
     "iopub.status.busy": "2021-06-02T13:53:08.148689Z",
     "iopub.status.idle": "2021-06-02T13:53:12.724394Z",
     "shell.execute_reply": "2021-06-02T13:53:12.723416Z",
     "shell.execute_reply.started": "2021-06-02T13:53:08.148985Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset[[\"Arabize\",\"Arabic\"]] = dataset[[\"Arabize\",\"Arabic\"]].apply(tokenize, axis=1)\n",
    "\n",
    "validation = dataset.sample(frac=0.1)\n",
    "train = dataset.drop(validation.index)\n",
    "\n",
    "X_train = train.Arabize\n",
    "y_train = train.Arabic\n",
    "\n",
    "X_valid = validation.Arabize\n",
    "y_valid = validation.Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:12.726342Z",
     "iopub.status.busy": "2021-06-02T13:53:12.725912Z",
     "iopub.status.idle": "2021-06-02T13:53:12.739607Z",
     "shell.execute_reply": "2021-06-02T13:53:12.738278Z",
     "shell.execute_reply.started": "2021-06-02T13:53:12.726298Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=9000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(\n",
    "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.scale * self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:12.741728Z",
     "iopub.status.busy": "2021-06-02T13:53:12.741388Z",
     "iopub.status.idle": "2021-06-02T13:53:12.759349Z",
     "shell.execute_reply": "2021-06-02T13:53:12.758216Z",
     "shell.execute_reply.started": "2021-06-02T13:53:12.741697Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, intoken, outtoken ,hidden, enc_layers=1, dec_layers=1, dropout=0.15, nheads=4):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        ff_model = hidden*4\n",
    "        \n",
    "        self.encoder = nn.Embedding(intoken, hidden)\n",
    "        self.pos_encoder = PositionalEncoding(hidden, dropout)\n",
    "\n",
    "        self.decoder = nn.Embedding(outtoken, hidden) \n",
    "        self.pos_decoder = PositionalEncoding(hidden, dropout)\n",
    "        \n",
    "        \n",
    "        encoder_layers = TransformerEncoderLayer(d_model=hidden, nhead = nheads, dim_feedforward = ff_model, dropout=dropout, activation='relu')\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, enc_layers)\n",
    "\n",
    "        encoder_layers = TransformerDecoderLayer(hidden, nheads, ff_model, dropout, activation='relu')\n",
    "        self.transformer_decoder = TransformerDecoder(encoder_layers, dec_layers)        \n",
    "\n",
    "        self.fc_out = nn.Linear(hidden, outtoken)\n",
    "\n",
    "        self.src_mask = None\n",
    "        self.trg_mask = None\n",
    "        self.memory_mask = None\n",
    "\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz, sz1=None):\n",
    "        \n",
    "        if sz1 == None:\n",
    "            mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        else:\n",
    "            mask = torch.triu(torch.ones(sz, sz1), 1)\n",
    "            \n",
    "        return mask.masked_fill(mask==1, float('-inf'))\n",
    "\n",
    "    def make_len_mask_enc(self, inp):\n",
    "        return (inp == pad_token).transpose(0, 1)   #(batch_size, output_seq_len)\n",
    "    \n",
    "    def make_len_mask_dec(self, inp):\n",
    "        return (inp == pad_token).transpose(0, 1) #(batch_size, input_seq_len)\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, src, trg): #SRC: (seq_len, batch_size)\n",
    "\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(trg.device)\n",
    "            \n",
    "\n",
    "        #Adding padding mask\n",
    "        src_pad_mask = self.make_len_mask_enc(src)\n",
    "        trg_pad_mask = self.make_len_mask_dec(trg)\n",
    "             \n",
    "\n",
    "        #Add embeddings Encoder\n",
    "        src = self.encoder(src)  #Embedding, (seq_len, batch_size, d_model)\n",
    "        src = self.pos_encoder(src)   #Pos embedding\n",
    "        \n",
    "        \n",
    "        #Add embedding decoder\n",
    "        trg = self.decoder(trg) #(seq_len, batch_size, d_model)\n",
    "        trg = self.pos_decoder(trg)\n",
    "\n",
    "        \n",
    "        memory = self.transformer_encoder(src, None, src_pad_mask)\n",
    "        output = self.transformer_decoder(tgt = trg, memory = memory, tgt_mask = self.trg_mask, memory_mask = None, \n",
    "                                          tgt_key_padding_mask = trg_pad_mask, memory_key_padding_mask = src_pad_mask)\n",
    "\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:12.761117Z",
     "iopub.status.busy": "2021-06-02T13:53:12.760806Z",
     "iopub.status.idle": "2021-06-02T13:53:12.777387Z",
     "shell.execute_reply": "2021-06-02T13:53:12.776267Z",
     "shell.execute_reply.started": "2021-06-02T13:53:12.761085Z"
    }
   },
   "outputs": [],
   "source": [
    "len(in_token_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:12.779577Z",
     "iopub.status.busy": "2021-06-02T13:53:12.779153Z",
     "iopub.status.idle": "2021-06-02T13:53:12.790113Z",
     "shell.execute_reply": "2021-06-02T13:53:12.789263Z",
     "shell.execute_reply.started": "2021-06-02T13:53:12.779537Z"
    }
   },
   "outputs": [],
   "source": [
    "len(out_token_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "model = TransformerModel(len(in_token_to_int), len(out_token_to_int), 128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arab2ArabizDS(Dataset):\n",
    "\n",
    "    def __init__(self, data, label):\n",
    "        \n",
    "        self.data = data.values.tolist()\n",
    "        self.labels = label.values.tolist()\n",
    "        \n",
    "        self.lengths_source = [len(i) for i in data]\n",
    "        self.lengths_label = [len(i) for i in label]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx], self.labels[idx], self.lengths_source[idx], self.lengths_label[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator_Arab2Arabiz(data):\n",
    "    \n",
    "    word, label, length_source, length_label = zip(*data)\n",
    "    \n",
    "    tensor_dim_1 = max(length_source)\n",
    "    tensor_dim_2 = max(length_label)\n",
    "    \n",
    "    out_word = torch.full((len(word), tensor_dim_1), dtype=torch.long, fill_value=pad_token)\n",
    "    label_word = torch.full((len(word), tensor_dim_2), dtype=torch.long, fill_value=pad_token)\n",
    "\n",
    "    for i in range(len(word)):\n",
    "        \n",
    "        out_word[i][:len(word[i])] = torch.Tensor(word[i])\n",
    "        label_word[i][:len(label[i])] = torch.Tensor(label[i])\n",
    "    \n",
    "    return (out_word, label_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, batch_size):\n",
    "        self.lens = [x[1] for x in data_source]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        idx = list(range(len(self.lens)))\n",
    "        arr = list(zip(self.lens, idx))\n",
    "\n",
    "        random.shuffle(arr)\n",
    "        n = self.batch_size*100\n",
    "\n",
    "        iterator = []\n",
    "\n",
    "        for i in range(0, len(self.lens), n):\n",
    "            dt = arr[i:i+n]\n",
    "            dt = sorted(dt, key=lambda x: x[0])\n",
    "\n",
    "            for j in range(0, len(dt), self.batch_size):\n",
    "                indices = list(map(lambda x: x[1], dt[j:j+self.batch_size]))\n",
    "                iterator.append(indices)\n",
    "\n",
    "        random.shuffle(iterator)\n",
    "        return iter([item for sublist in iterator for item in sublist])  #Flatten nested list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_data = Arab2ArabizDS(X_train, y_train)\n",
    "train_sampler = KSampler(train_data, batch_size)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, worker_init_fn=seed_worker, collate_fn=data_collator_Arab2Arabiz)\n",
    "\n",
    "valid_data = Arab2ArabizDS(X_valid, y_valid)\n",
    "valid_sampler = KSampler(valid_data, batch_size)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size,worker_init_fn=seed_worker, collate_fn=data_collator_Arab2Arabiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
    "optimizer = NoamOpt(128, 1, 4000 ,optim.Adam(model.parameters(), lr=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(iterator):\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    for src, trg in iterator:\n",
    "\n",
    "        src = src.T.to(device)\n",
    "        trg = trg.T.to(device)\n",
    "\n",
    "        output = model(src, trg[:-1, :])\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "\n",
    "        optimizer.optimizer.zero_grad()\n",
    "        loss = criterion(output, trg[1:].reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(iterator):\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    for src, trg in iterator:\n",
    "\n",
    "        src = src.T.to(device)\n",
    "        trg = trg.T.to(device)\n",
    "\n",
    "        output = model(src, trg[:-1, :])\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "\n",
    "        optimizer.optimizer.zero_grad()\n",
    "        loss = criterion(output, trg[1:].reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "\n",
    "min_loss = 99\n",
    "#Change model size \n",
    "for i in range(100):\n",
    "    \n",
    "    loss = run_epoch(train_dataloader)\n",
    "    loss_val = run_validation(valid_dataloader)\n",
    "    \n",
    "    if loss_val < min_loss:\n",
    "        min_loss = loss_val\n",
    "        torch.save(model, \"convert_best\")\n",
    "    \n",
    "    print(\"EPOCH %d -- %f -- Val Loss: %f\" % (i, loss, loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"convert_best\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_int_to_token = {out_token_to_int[t]:t for t in out_token_to_int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabizi_2_arabic(inp):\n",
    "    \n",
    "    input_sentence = [in_token_to_int[i] for i in inp.lower()]\n",
    "    preds = [sos_token]\n",
    "\n",
    "    input_sentence = torch.Tensor(input_sentence).unsqueeze(-1).long().to(device)\n",
    "\n",
    "\n",
    "    new_char = -1\n",
    "\n",
    "    while new_char != eos_token:\n",
    "\n",
    "        output_sentence = torch.Tensor(preds).unsqueeze(-1).long().to(device)\n",
    "\n",
    "        src = model.pos_encoder(model.encoder(input_sentence))\n",
    "        trg = model.pos_encoder(model.decoder(output_sentence))\n",
    "\n",
    "        memory = model.transformer_encoder(src)\n",
    "        output = model.transformer_decoder(tgt = trg, memory = memory)\n",
    "\n",
    "        output = model.fc_out(output)\n",
    "        new_char = output.argmax(-1)[-1, 0].item()\n",
    "\n",
    "        preds.append(new_char)\n",
    "\n",
    "        if len(preds) > 50:\n",
    "            break\n",
    "        \n",
    "\n",
    "    return \"\".join([out_int_to_token[i] for i in preds[1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/zindidd/Train.csv\")[[\"textt\", \"labell\"]]\n",
    "train.columns = [\"texts\", \"data_labels\"]\n",
    "\n",
    "data = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):    #Might use the same setting if they work to other languages (english and french)  \n",
    "\n",
    "    text = text.replace('ß',\"b\")\n",
    "    text = text.replace('à',\"a\")\n",
    "    text = text.replace('á',\"a\")\n",
    "    text = text.replace('ç',\"c\")\n",
    "    text = text.replace('è',\"e\")\n",
    "    text = text.replace('é',\"e\")\n",
    "    text = text.replace('$',\"s\")\n",
    "    text = text.replace(\"1\",\"\")\n",
    "    \n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^A-Za-z0-9 ,!?.]', '', text)\n",
    "\n",
    "    \n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    text = re.sub(r'([h][h][h][h])\\1+', r'\\1', text)\n",
    "    text = re.sub(r'([a-g-i-z])\\1+', r'\\1', text)  #Remove repeating characters\n",
    "    text = re.sub(r' [0-9]+ ', \" \", text)\n",
    "    text = re.sub(r'^[0-9]+ ', \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep numbers block\n",
    "def split(text):\n",
    "    \n",
    "    splits = re.findall(r\"[\\w']+|[?!.,]\", text)\n",
    "\n",
    "    to_be_added = []\n",
    "    idx_to_be_added = []\n",
    "    \n",
    "    forbidden = [\"?\", \"!\", \".\", \",\"] + known_idx\n",
    "\n",
    "    for i, split in enumerate(splits):\n",
    "\n",
    "        if split in forbidden:\n",
    "            if split in known_idx:\n",
    "                to_be_added.append(known[split])\n",
    "            else:\n",
    "                to_be_added.append(split)\n",
    "            idx_to_be_added.append(i)\n",
    "        #else:\n",
    "        #splits[i] = splits[i][:1000]\n",
    "\n",
    "\n",
    "    splits = [i for i in splits if not i in forbidden]\n",
    "    \n",
    "    return splits, to_be_added, idx_to_be_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic = []\n",
    "\n",
    "def convert_phrase_2(text):\n",
    "    text = text.replace(\"0\",\"\")\n",
    "    text = text.replace(\"6\",\"\")\n",
    "\n",
    "    #print(\"\\nTEXT: \"+text)\n",
    "    phrase, to_be_added, idx_to_be_added = split(text.lower())\n",
    "\n",
    "    max_len_phrase = max([len(i) for i in phrase])\n",
    "\n",
    "    input_sentence = []\n",
    "    for word in phrase:\n",
    "        input_sentence.append([in_token_to_int[i] for i in word] + [pad_token]*(max_len_phrase-len(word)))\n",
    "\n",
    "    input_sentence = torch.Tensor(input_sentence).long().T.to(device)\n",
    "    preds = [[sos_token] * len(phrase)]\n",
    "\n",
    "    end_word = len(phrase) * [False]\n",
    "    src_pad_mask = model.make_len_mask_enc(input_sentence)\n",
    "\n",
    "\n",
    "    while not all(end_word):\n",
    "        output_sentence = torch.Tensor(preds).long().to(device)\n",
    "\n",
    "        src = model.pos_encoder(model.encoder(input_sentence))\n",
    "        trg = model.pos_encoder(model.decoder(output_sentence))\n",
    "\n",
    "        memory = model.transformer_encoder(src, None ,src_pad_mask)\n",
    "        output = model.transformer_decoder(tgt = trg, memory = memory, memory_key_padding_mask = src_pad_mask)\n",
    "        \n",
    "        \n",
    "        output = model.fc_out(output)\n",
    "\n",
    "\n",
    "        output = output.argmax(-1)[-1].cpu().detach().numpy()\n",
    "        preds.append(output.tolist())\n",
    "\n",
    "\n",
    "        end_word = (output == eos_token) | end_word\n",
    "        \n",
    "        if len(preds) > 50:\n",
    "            global problematic \n",
    "            \n",
    "            problematic.append(text)\n",
    "            #print(text)\n",
    "            break\n",
    "            \n",
    "    \n",
    "    preds = np.array(preds).T\n",
    "    result = []\n",
    "\n",
    "    for word in preds:\n",
    "\n",
    "        tmp = []\n",
    "        for i in word[1:]:   \n",
    "            if out_int_to_token[i] == \"<eos>\":\n",
    "                break\n",
    "            tmp.append(out_int_to_token[i])\n",
    "\n",
    "        result.append(\"\".join(tmp))\n",
    "\n",
    "        \n",
    "    #Re-add removed punctuation\n",
    "    for item, idx in zip(to_be_added, idx_to_be_added):\n",
    "\n",
    "        if item == \"?\":\n",
    "            item = \"؟\"\n",
    "        elif item == \",\":\n",
    "            item = \"،\"\n",
    "\n",
    "        result.insert(idx, item)\n",
    "        \n",
    "        \n",
    "    result = \" \".join(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.texts = train.texts.apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "step_size = 100\n",
    "\n",
    "texts = train.texts.values.tolist()\n",
    "\n",
    "for i in tqdm(range(0, len(texts), step_size)): \n",
    "    \n",
    "    out = convert_phrase_2(\" lkrb3 \".join(texts[i:i+step_size]))\n",
    "    splitted_sentences = [ex.lstrip().rstrip() for ex in out.split(\" \" + convert_phrase_2(\"lkrb3\") + \" \")]\n",
    "    \n",
    "    if len(splitted_sentences) != len(texts[i:i+step_size]):\n",
    "        print(\"DANGER\")\n",
    "        break\n",
    "    \n",
    "    results.extend(splitted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"converted\"] = results.copy()\n",
    "train.to_csv(\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../input/zindidd/Test.csv\")\n",
    "test.textt = test.textt.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "step_size = 50\n",
    "\n",
    "texts = test.textt.values.tolist()\n",
    "\n",
    "for i in tqdm(range(0, len(texts), step_size)): \n",
    "    \n",
    "    out = convert_phrase_2(\" lkrb3 \".join(texts[i:i+step_size]))\n",
    "    splitted_sentences = [ex.lstrip().rstrip() for ex in out.split(\" \" + convert_phrase_2(\"lkrb3\") + \" \")]\n",
    "    \n",
    "    if len(splitted_sentences) != len(texts[i:i+step_size]):\n",
    "        print(\"DANGER\")\n",
    "        break\n",
    "    \n",
    "    results.extend(splitted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"converted\"] = results\n",
    "test.to_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data, tokenizer, preprocess_text, max_len=256):\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    tmp = tokenizer.encode(\"ab\")[-1]\n",
    "\n",
    "    for sentence in data:\n",
    "\n",
    "        encoding = tokenizer.encode(preprocess_text(sentence))\n",
    "\n",
    "        if len(encoding) > max_len:\n",
    "            encoding = encoding[:max_len-1] + [tmp]\n",
    "\n",
    "        in_ids = encoding\n",
    "        att_mask = [1]*len(encoding)\n",
    "        \n",
    "        input_ids.append(in_ids)\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, masks, label=None):\n",
    "        \n",
    "        self.data = data\n",
    "        self.masks = masks\n",
    "        \n",
    "        if label != None:\n",
    "            self.labels = label\n",
    "        else:\n",
    "            self.labels = None\n",
    "        \n",
    "        self.lengths = [len(i) for i in data]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels !=  None:\n",
    "            return (self.data[idx], self.masks[idx], self.labels[idx], self.lengths[idx])\n",
    "        else:  #For validation\n",
    "            return (self.data[idx], self.masks[idx], None, self.lengths[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(data):\n",
    "    \n",
    "    sentence, mask, label, length = zip(*data)\n",
    "    \n",
    "    tensor_dim = max(length)\n",
    "    \n",
    "    out_sentence = torch.full((len(sentence), tensor_dim), dtype=torch.long, fill_value=pad)\n",
    "    out_mask = torch.zeros(len(sentence), tensor_dim, dtype=torch.long)\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        \n",
    "        out_sentence[i][:len(sentence[i])] = torch.Tensor(sentence[i])\n",
    "        out_mask[i][:len(mask[i])] = torch.Tensor(mask[i])\n",
    "    \n",
    "    if label[0] != None:\n",
    "        return (out_sentence, out_mask, torch.Tensor(label).long())\n",
    "    else:\n",
    "        return (out_sentence, out_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, batch_size):\n",
    "        self.lens = [x[1] for x in data_source]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        idx = list(range(len(self.lens)))\n",
    "        arr = list(zip(self.lens, idx))\n",
    "\n",
    "        random.shuffle(arr)\n",
    "        n = self.batch_size*100\n",
    "\n",
    "        iterator = []\n",
    "\n",
    "        for i in range(0, len(self.lens), n):\n",
    "            dt = arr[i:i+n]\n",
    "            dt = sorted(dt, key=lambda x: x[0])\n",
    "\n",
    "            for j in range(0, len(dt), self.batch_size):\n",
    "                indices = list(map(lambda x: x[1], dt[j:j+self.batch_size]))\n",
    "                iterator.append(indices)\n",
    "\n",
    "        random.shuffle(iterator)\n",
    "        return iter([item for sublist in iterator for item in sublist])  #Flatten nested list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, model_name, dropout, freeze_bert=False):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "        D_in, H, D_out = 768, 200, 3\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, epochs=4, dropout=0.1):\n",
    "\n",
    "    bert_classifier = BertClassifier(model_name, dropout=dropout, freeze_bert=False)\n",
    "\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,   \n",
    "                      eps=1e-8 \n",
    "                      )\n",
    "\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False, fold=0, prefix=\"\"):\n",
    "    \n",
    "    global max_acc\n",
    "\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "                \n",
    "            if step%200 == 0 and step != 0 and epoch_i != 0 and epoch_i != 1:\n",
    "                \n",
    "                print(\"-\"*70)\n",
    "\n",
    "                if evaluation == True:\n",
    "\n",
    "                    val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "                    \n",
    "                    if val_accuracy > max_acc:\n",
    "                        max_acc = val_accuracy\n",
    "                        torch.save(model, prefix + \"_best_\"+str(fold))\n",
    "                        print(\"new max\")\n",
    "                        \n",
    "\n",
    "                    print(val_accuracy)\n",
    "                    \n",
    "                    print(\"-\"*70)\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                model.train()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        if evaluation == True:\n",
    "            \n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "            \n",
    "            if val_accuracy > max_acc:\n",
    "                max_acc = val_accuracy\n",
    "                torch.save(model, prefix+\"_best_\"+str(fold))\n",
    "                print(\"new max\")\n",
    "\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(arr, idxs):  #Helper function to get multiple indexes from a list\n",
    "    \n",
    "    output = []\n",
    "    for idx in idxs:\n",
    "        output.append(arr[idx])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tried these different preprocessing functions and tesed their effect on the results\n",
    "#Found out that text_preprocessing_2 gives the best results for the English model\n",
    "def text_preprocessing_1(text): \n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "    return text\n",
    "\n",
    "\n",
    "def text_preprocessing_2(text): \n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    text = re.sub(r'([a-g-i-z][a-g-i-z])\\1+', r'\\1', text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "def text_preprocessing_3(text):    \n",
    "\n",
    "    text = text.replace('ß',\"b\")\n",
    "    text = text.replace('à',\"a\")\n",
    "    text = text.replace('á',\"a\")\n",
    "    text = text.replace('ç',\"c\")\n",
    "    text = text.replace('è',\"e\")\n",
    "    text = text.replace('é',\"e\")\n",
    "    text = text.replace('$',\"s\")\n",
    "    text = text.replace(\"1\",\"\")\n",
    "    \n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^A-Za-z0-9 ,!?.]', '', text)\n",
    "\n",
    "    \n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    text = re.sub(r'([h][h][h][h])\\1+', r'\\1', text)\n",
    "    text = re.sub(r'([a-g-i-z])\\1+', r'\\1', text)  #Remove repeating characters\n",
    "    text = re.sub(r' [0-9]+ ', \" \", text)\n",
    "    text = re.sub(r'^[0-9]+ ', \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../input/zindidd/Train.csv\")[[\"textt\", \"labell\"]].iloc[1000:]\n",
    "data.columns = [\"texts\", \"data_labels\"]\n",
    "\n",
    "data.data_labels = data.data_labels.replace(0,2)  #Neutral 2, Positive 1, Negative 0\n",
    "data.data_labels = data.data_labels.replace(-1,0)\n",
    "\n",
    "\n",
    "\n",
    "X = data.texts.values\n",
    "y = data.data_labels.values\n",
    "\n",
    "preprocessed_data, masks = preprocessing_for_bert(X, tokenizer_en, text_preprocessing_2, max_len=256)\n",
    "pad = tokenizer_en.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(5, True, seed)\n",
    "fold = 0\n",
    "\n",
    "bests = []\n",
    "\n",
    "for train_ids, val_ids in kfold.split(preprocessed_data):\n",
    "    \n",
    "    print(\"\\n\\tFOLD %d \\n\" % (fold))\n",
    "    max_acc = -99\n",
    "\n",
    "    X_train = get_indices(preprocessed_data, train_ids)\n",
    "    y_train = get_indices(y, train_ids)\n",
    "    train_masks = get_indices(masks, train_ids)\n",
    "    \n",
    "    X_val = get_indices(preprocessed_data, val_ids)\n",
    "    y_val = get_indices(y, val_ids)\n",
    "    val_masks = get_indices(masks, val_ids)\n",
    "    \n",
    "    \n",
    "    X_val, y_val, val_masks = list(zip(*sorted(zip(X_val, y_val, val_masks), key=lambda x: len(x[0]))))  #Order the validation data for faster validation\n",
    "    X_val, y_val, val_masks = list(X_val), list(y_val), list(val_masks)\n",
    "    \n",
    "    \n",
    "    # Convert other data types to torch.Tensor\n",
    "    y_train = torch.tensor(y_train)\n",
    "    y_val = torch.tensor(y_val)\n",
    "\n",
    "    # Create the DataLoader for our training set\n",
    "    train_data = BertDataset(X_train, train_masks, y_train)\n",
    "    train_sampler = KSampler(train_data, batch_size)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "    # Create the DataLoader for our validation set\n",
    "    val_data = BertDataset(X_val, val_masks, y_val)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size, collate_fn=data_collator)\n",
    "    \n",
    "    \n",
    "    set_seed()    # Set seed for reproducibility\n",
    "    bert_classifier, optimizer, scheduler = initialize_model(model_name=model_name_en, epochs=n_epochs, dropout=0.05)\n",
    "    train(bert_classifier, train_dataloader, val_dataloader, epochs=n_epochs, evaluation=True, fold=fold, prefix=\"en\")\n",
    "    \n",
    "    fold += 1\n",
    "    bests.append(max_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train_data.csv\")[[\"converted\", \"data_labels\"]].iloc[1000:]\n",
    "data.columns = [\"texts\", \"data_labels\"]\n",
    "\n",
    "data.data_labels = data.data_labels.replace(0,2)  #Neutral 2, Positive 1, Negative 0\n",
    "data.data_labels = data.data_labels.replace(-1,0)\n",
    "\n",
    "\n",
    "\n",
    "X = data.texts.values\n",
    "y = data.data_labels.values\n",
    "\n",
    "preprocessed_data, masks = preprocessing_for_bert(X, tokenizer_ar, lambda x: x, max_len=256)\n",
    "pad = tokenizer_ar.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(10, True, seed)\n",
    "fold = 0\n",
    "\n",
    "bests = []\n",
    "\n",
    "for train_ids, val_ids in kfold.split(preprocessed_data):\n",
    "    \n",
    "    print(\"\\n\\tFOLD %d \\n\" % (fold))\n",
    "    max_acc = -99\n",
    "    \n",
    "    X_train = get_indices(preprocessed_data, train_ids)\n",
    "    y_train = get_indices(y, train_ids)\n",
    "    train_masks = get_indices(masks, train_ids)\n",
    "    \n",
    "    X_val = get_indices(preprocessed_data, val_ids)\n",
    "    y_val = get_indices(y, val_ids)\n",
    "    val_masks = get_indices(masks, val_ids)\n",
    "    \n",
    "\n",
    "    X_val, y_val, val_masks = list(zip(*sorted(zip(X_val, y_val, val_masks), key=lambda x: len(x[0]))))  #Order the validation data for faster validation\n",
    "    X_val, y_val, val_masks = list(X_val), list(y_val), list(val_masks)\n",
    "    \n",
    "    \n",
    "    # Convert other data types to torch.Tensor\n",
    "    y_train = torch.tensor(y_train)\n",
    "    y_val = torch.tensor(y_val)\n",
    "\n",
    "    # Create the DataLoader for our training set\n",
    "    train_data = BertDataset(X_train, train_masks, y_train)\n",
    "    train_sampler = KSampler(train_data, batch_size)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "    # Create the DataLoader for our validation set\n",
    "    val_data = BertDataset(X_val, val_masks, y_val)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size, collate_fn=data_collator)\n",
    "    \n",
    "    \n",
    "    set_seed()    # Set seed for reproducibility\n",
    "    bert_classifier, optimizer, scheduler = initialize_model(model_name=model_name_ar, epochs=n_epochs, dropout=0)\n",
    "    train(bert_classifier, train_dataloader, val_dataloader, epochs=n_epochs, evaluation=True, fold=fold, prefix=\"ar\")\n",
    "    \n",
    "    fold += 1\n",
    "    bests.append(max_acc)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_single_predict(model, test_dataloader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    for batch in tqdm(test_dataloader):\n",
    "\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_ensemble_predict(sentences, models, tokenizer, preprocess, truncate=True, max_len=256):\n",
    "    \n",
    "    inputs, masks = preprocessing_for_bert(sentences, tokenizer, preprocess, max_len=max_len)\n",
    "    \n",
    "    \n",
    "    dataset = BertDataset(inputs, masks)\n",
    "    sample = SequentialSampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler=sample, batch_size=128, collate_fn=data_collator)\n",
    "    \n",
    "    preds = []\n",
    "    \n",
    "    for model in models:\n",
    "        preds.append(bert_single_predict(model, dataloader))\n",
    "        \n",
    "    return preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lang(lang_prefix, directory, preprocess_fn, dataset, model_name, n=1, truncate=True, max_len=256):\n",
    "    \n",
    "    print(\"Loading the models ....\")\n",
    "    \n",
    "    global pad\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "    pad = tokenizer.pad_token_id\n",
    "    \n",
    "    lang_models = []\n",
    "    for i in range(n):\n",
    "        lang_models.append(torch.load(directory + \"/\" + lang_prefix + \"best_\"+str(i), map_location=device))\n",
    "        \n",
    "    print(\"Inference ....\")\n",
    "\n",
    "    out = bert_ensemble_predict(dataset, lang_models, tokenizer, preprocess_fn, truncate=truncate, max_len=max_len)\n",
    "\n",
    "    out_sum = out[0]\n",
    "    for i in range(1,n):\n",
    "        out_sum = out[i] + out_sum\n",
    "    \n",
    "    return out_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the list for faster inference\n",
    "df = pd.read_csv(\"../input/zindidd/Test.csv\")\n",
    "df_converted = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "df[\"lens\"] = df.textt.apply(len)\n",
    "df = df.sort_values(by=\"lens\").set_index(\"IDD\", drop=True)\n",
    "df_converted = df_converted.set_index(\"IDD\", drop=True).loc[df.index]\n",
    "\n",
    "\n",
    "#Convert to list\n",
    "test = df.textt.tolist()\n",
    "test_converted = df_converted[[\"converted\"]].converted.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ar = predict_lang(\"ar_\", \"./\", lambda x:x, test_converted, model_name_ar, n=10, truncate=True, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_en = predict_lang(\"en_\", \"./\", text_preprocessing_2, test, model_name_en, n=5, truncate=True, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"preds\"] = ((output_ar/10)*1.30+(output_en/5)).argmax(1)\n",
    "\n",
    "df.preds = df.preds.replace(0,-1)\n",
    "df.preds = df.preds.replace(2,0)\n",
    "\n",
    "the_output = df.reset_index()[[\"IDD\", \"preds\"]]\n",
    "the_output.columns = [\"ID\", \"label\"]\n",
    "\n",
    "the_output.to_csv(\"lessvalid_convvalid150.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
